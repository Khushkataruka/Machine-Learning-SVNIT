{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c44f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from-scratch GridSearch for MyRidge...\n",
      "GridSearch complete. Best alpha: 100 with score: 0.6504\n",
      "Running from-scratch GridSearch for MyLasso...\n",
      "GridSearch complete. Best alpha: 10 with score: 0.6577\n",
      "\n",
      "Training final Ridge model...\n",
      "Training final Lasso model...\n",
      "Training doubled-alpha models...\n",
      "\n",
      "==================================================\n",
      "           FINAL RESULTS (FROM SCRATCH)           \n",
      "==================================================\n",
      "\n",
      "Question 1: Optimal Alphas (from scratch GridSearch)\n",
      "  - Optimal alpha for Ridge: 100\n",
      "  - Optimal alpha for Lasso: 10\n",
      "--------------------------------------------------\n",
      "Question 2: Effect of Doubling Alpha\n",
      "\n",
      "--- Performance on Test Set (using original SalePrice) ---\n",
      "  - Ridge (alpha=100):\n",
      "    R²: 0.8500 | RMSE: $32,355.73 | Features Used: 266\n",
      "  - Ridge (alpha=200):\n",
      "    R²: 0.8759 | RMSE: $29,424.61 | Features Used: 266\n",
      "  - Lasso (alpha=10):\n",
      "    R²: 0.8604 | RMSE: $31,207.11 | Features Used: 266\n",
      "  - Lasso (alpha=20):\n",
      "    R²: 0.8043 | RMSE: $36,951.88 | Features Used: 266\n",
      "\n",
      "  Analysis of Doubling Alpha:\n",
      "  - For Ridge, doubling alpha increases the penalty, shrinking all coefficients further.\n",
      "  - For Lasso, doubling alpha also increases the penalty, forcing more coefficients to exactly zero (see 'Features Used').\n",
      "--------------------------------------------------\n",
      "Question 3: Significant Variables (from optimal Lasso model)\n",
      "  - Lasso (alpha=10) selected 266 out of 266 features.\n",
      "\n",
      "  - Top 15 Most Important Predictors:\n",
      "OverallQual         0.127577\n",
      "GrLivArea           0.097430\n",
      "GarageCars          0.061496\n",
      "YearRemodAdd        0.029896\n",
      "YearBuilt           0.025146\n",
      "MSZoning_RM        -0.025085\n",
      "CentralAir_Y        0.019773\n",
      "BsmtFullBath        0.015801\n",
      "Fireplaces          0.013814\n",
      "TotalBsmtSF         0.012795\n",
      "FireplaceQu_None   -0.012141\n",
      "OverallCond         0.011428\n",
      "PoolQC_Gd          -0.010534\n",
      "MSSubClass_160     -0.009894\n",
      "LotArea             0.009149\n",
      "--------------------------------------------------\n",
      "Question 4: How well do the variables describe the price?\n",
      "  This is answered by the R² and RMSE metrics on our test set.\n",
      "  - Our best from-scratch Lasso model explains 86.0% of the variance in house prices.\n",
      "  - Its average prediction error (RMSE) is $31,207.11.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "# We only use train_test_split from sklearn for a simple data split\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# PART 1: METRIC FUNCTIONS (FROM SCRATCH)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def my_rmse(y_true, y_pred):\n",
    "    \"\"\"Calculates Root Mean Squared Error.\"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def my_r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculates R-Squared (R²).\"\"\"\n",
    "    # Sum of squares of residuals\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    # Total sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    \n",
    "    if ss_tot == 0:\n",
    "        return 1.0 # Perfect model or no variance\n",
    "    \n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# PART 2: PREPROCESSING CLASS (FROM SCRATCH)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "class MyStandardScaler:\n",
    "    \"\"\"Implements StandardScaler from scratch.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Calculates and stores the mean and std dev of each feature.\"\"\"\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0)\n",
    "        # Avoid division by zero for constant features\n",
    "        self.std_[self.std_ == 0] = 1.0\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Applies the standardization using the stored mean and std.\"\"\"\n",
    "        if self.mean_ is None or self.std_ is None:\n",
    "            raise ValueError(\"Must call fit() before transform()\")\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"A helper function to do both fit and transform.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# PART 3: MODEL CLASSES (FROM SCRATCH)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "class MyLinearRegressionBase:\n",
    "    \"\"\"\n",
    "    Base class for Linear Regression models, using Gradient Descent.\n",
    "    This class is not meant to be used directly.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, alpha=0.1):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.alpha = alpha  # Regularization parameter\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _init_params(self, n_features):\n",
    "        \"\"\"Initialize weights and bias.\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict y values for new X.\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def _base_cost_gradient(self, X, y_true, y_pred):\n",
    "        \"\"\"Gradients for Mean Squared Error.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        # Gradient of MSE w.r.t weights (dw) and bias (db)\n",
    "        dw = (2 / n_samples) * np.dot(X.T, (y_pred - y_true))\n",
    "        db = (2 / n_samples) * np.sum(y_pred - y_true)\n",
    "        return dw, db\n",
    "\n",
    "    # --- Methods to be overridden by child classes ---\n",
    "    def _regularization_gradient(self):\n",
    "        \"\"\"Placeholder for regularization penalty on gradient.\"\"\"\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trains the model using gradient descent.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self._init_params(n_features)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            # 1. Get current predictions\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # 2. Get base MSE gradients\n",
    "            dw, db = self._base_cost_gradient(X, y, y_pred)\n",
    "            \n",
    "            # 3. Add regularization gradients (from child class)\n",
    "            dw += self._regularization_gradient()\n",
    "            \n",
    "            # 4. Update weights and bias\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "class MyRidge(MyLinearRegressionBase):\n",
    "    \"\"\"Implements Ridge Regression (L2 Penalty) from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, alpha=0.1):\n",
    "        super().__init__(learning_rate, n_iters, alpha)\n",
    "        \n",
    "    def _regularization_gradient(self):\n",
    "        \"\"\"L2 penalty for gradient.\"\"\"\n",
    "        # Gradient of L2 = (alpha / N) * w\n",
    "        # We add regularization to the gradient\n",
    "        return (self.alpha / len(self.weights)) * self.weights\n",
    "\n",
    "class MyLasso(MyLinearRegressionBase):\n",
    "    \"\"\"Implements Lasso Regression (L1 Penalty) from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, alpha=0.1):\n",
    "        super().__init__(learning_rate, n_iters, alpha)\n",
    "        \n",
    "    def _regularization_gradient(self):\n",
    "        \"\"\"\n",
    "        L1 penalty for gradient (sub-gradient).\n",
    "        Gradient of L1 = (alpha / N) * sign(w)\n",
    "        \"\"\"\n",
    "        return (self.alpha / len(self.weights)) * np.sign(self.weights)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# PART 4: HYPERPARAMETER TUNING (FROM SCRATCH)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def my_k_fold_indices(n_samples, k_folds):\n",
    "    \"\"\"\n",
    "    From-scratch K-Fold index generator.\n",
    "    Yields (train_indices, validation_indices) for each fold.\n",
    "    \"\"\"\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_sizes = np.full(k_folds, n_samples // k_folds, dtype=int)\n",
    "    fold_sizes[:n_samples % k_folds] += 1\n",
    "    \n",
    "    current = 0\n",
    "    for fold_size in fold_sizes:\n",
    "        start, end = current, current + fold_size\n",
    "        val_idx = indices[start:end]\n",
    "        train_idx = np.concatenate((indices[:start], indices[end:]))\n",
    "        yield train_idx, val_idx\n",
    "        current = end\n",
    "\n",
    "def my_grid_search_cv(model_class, param_grid, X, y, cv=5, scoring='r2'):\n",
    "    \"\"\"\n",
    "    Implements GridSearchCV from scratch.\n",
    "    \n",
    "    model_class: The class to instantiate (e.g., MyRidge, MyLasso)\n",
    "    param_grid: A dict, e.g., {'alpha': [0.1, 1, 10]}\n",
    "    X, y: *Already scaled* full training data\n",
    "    \"\"\"\n",
    "    print(f\"Running from-scratch GridSearch for {model_class.__name__}...\")\n",
    "    \n",
    "    alphas = param_grid['alpha']\n",
    "    best_score = -np.inf\n",
    "    best_alpha = None\n",
    "    \n",
    "    # We use lower iters/lr for grid search to speed it up\n",
    "    search_lr = 0.01\n",
    "    search_iters = 500\n",
    "\n",
    "    for alpha in alphas:\n",
    "        fold_scores = []\n",
    "        \n",
    "        # We use the *already scaled* data and split it\n",
    "        for train_idx, val_idx in my_k_fold_indices(len(X), cv):\n",
    "            # 1. Get fold data\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # 2. Instantiate and train model\n",
    "            model = model_class(learning_rate=search_lr, n_iters=search_iters, alpha=alpha)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # 3. Predict and score\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            \n",
    "            # 4. Reverse log-transform for scoring\n",
    "            y_pred_orig = np.expm1(y_pred_fold)\n",
    "            y_val_orig = np.expm1(y_val_fold)\n",
    "            \n",
    "            if scoring == 'r2':\n",
    "                score = my_r2_score(y_val_orig, y_pred_orig)\n",
    "            else: # Default to RMSE (lower is better)\n",
    "                score = -my_rmse(y_val_orig, y_pred_orig) # Negate so higher is better\n",
    "                \n",
    "            fold_scores.append(score)\n",
    "        \n",
    "        # 5. Average score for this alpha\n",
    "        avg_score = np.mean(fold_scores)\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_alpha = alpha\n",
    "            \n",
    "    print(f\"GridSearch complete. Best alpha: {best_alpha} with score: {best_score:.4f}\")\n",
    "    return {'alpha': best_alpha}\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# PART 5: MAIN SCRIPT (PIPELINE)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- 1. Load and Prepare Data (Using pandas for this part) ---\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv not found.\")\n",
    "    # In a real script, you'd exit() here\n",
    "    # For this example, we'll assume df is loaded.\n",
    "    pass\n",
    "\n",
    "# Fill 'NA' = 'None' for specific columns\n",
    "na_means_none_cols = [\n",
    "    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "    'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
    "    'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'\n",
    "]\n",
    "for col in na_means_none_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('None')\n",
    "\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 2. Log-transform target variable (improves model stability) ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "# y_test (original) is kept for final scoring\n",
    "# y_test_log is not needed since we don't train on it\n",
    "\n",
    "# --- 3. Preprocessing (pandas for imputation/dummies) ---\n",
    "X_train['MSSubClass'] = X_train['MSSubClass'].astype(str)\n",
    "X_test['MSSubClass'] = X_test['MSSubClass'].astype(str)\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=np.number).columns.drop('Id')\n",
    "categorical_cols = X_train.select_dtypes(include='object').columns\n",
    "\n",
    "# Imputation\n",
    "for col in numeric_cols:\n",
    "    if col in X_train.columns:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train.loc[:, col] = X_train[col].fillna(median_val)\n",
    "        X_test.loc[:, col] = X_test[col].fillna(median_val)\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        mode_val = X_train[col].mode()[0]\n",
    "        X_train.loc[:, col] = X_train[col].fillna(mode_val)\n",
    "        X_test.loc[:, col] = X_test[col].fillna(mode_val)\n",
    "\n",
    "X_train_dummified = pd.get_dummies(X_train.drop('Id', axis=1, errors='ignore'), drop_first=True)\n",
    "X_test_dummified = pd.get_dummies(X_test.drop('Id', axis=1, errors='ignore'), drop_first=True)\n",
    "X_train_final, X_test_final = X_train_dummified.align(X_test_dummified, join='left', axis=1, fill_value=0)\n",
    "\n",
    "feature_names = X_train_final.columns\n",
    "\n",
    "# --- 4. Convert to NumPy arrays for our from-scratch classes ---\n",
    "#\n",
    "# ----- THIS IS THE FIX -----\n",
    "# Force the dtype to float64 to ensure np.std and np.mean work\n",
    "X_train_np = X_train_final.to_numpy(dtype=np.float64)\n",
    "X_test_np = X_test_final.to_numpy(dtype=np.float64)\n",
    "# -------------------------\n",
    "y_train_log_np = y_train_log.to_numpy()\n",
    "\n",
    "# --- 5. Scale Data (Using our from-scratch scaler) ---\n",
    "# We fit on the training data\n",
    "scaler_final = MyStandardScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train_np)\n",
    "# We transform the test data\n",
    "X_test_scaled = scaler_final.transform(X_test_np)\n",
    "\n",
    "# --- 6. Find Optimal Alphas (From Scratch) ---\n",
    "alphas_to_test = [0.1, 1, 10, 100, 200] # Smaller list for speed\n",
    "param_grid = {'alpha': alphas_to_test}\n",
    "\n",
    "# Pass the *scaled* training data to the grid search\n",
    "ridge_params = my_grid_search_cv(MyRidge, param_grid, X_train_scaled, y_train_log_np, cv=3)\n",
    "lasso_params = my_grid_search_cv(MyLasso, param_grid, X_train_scaled, y_train_log_np, cv=3)\n",
    "\n",
    "best_alpha_ridge = ridge_params['alpha']\n",
    "best_alpha_lasso = lasso_params['alpha']\n",
    "\n",
    "# --- 7. Train Final Models (From Scratch) ---\n",
    "final_iters = 2000\n",
    "final_lr = 0.01\n",
    "\n",
    "print(\"\\nTraining final Ridge model...\")\n",
    "best_ridge = MyRidge(learning_rate=final_lr, n_iters=final_iters, alpha=best_alpha_ridge)\n",
    "best_ridge.fit(X_train_scaled, y_train_log_np)\n",
    "\n",
    "print(\"Training final Lasso model...\")\n",
    "best_lasso = MyLasso(learning_rate=final_lr, n_iters=final_iters, alpha=best_alpha_lasso)\n",
    "best_lasso.fit(X_train_scaled, y_train_log_np)\n",
    "\n",
    "# Train doubled-alpha models\n",
    "print(\"Training doubled-alpha models...\")\n",
    "doubled_ridge = MyRidge(learning_rate=final_lr, n_iters=final_iters, alpha=best_alpha_ridge * 2)\n",
    "doubled_ridge.fit(X_train_scaled, y_train_log_np)\n",
    "\n",
    "doubled_lasso = MyLasso(learning_rate=final_lr, n_iters=final_iters, alpha=best_alpha_lasso * 2)\n",
    "doubled_lasso.fit(X_train_scaled, y_train_log_np)\n",
    "\n",
    "# --- 8. Final Evaluation and Answers ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           FINAL RESULTS (FROM SCRATCH)           \")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Question 1: Optimal Alphas ---\n",
    "print(f\"Question 1: Optimal Alphas (from scratch GridSearch)\")\n",
    "print(f\"  - Optimal alpha for Ridge: {best_alpha_ridge}\")\n",
    "print(f\"  - Optimal alpha for Lasso: {best_alpha_lasso}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Question 2: Effect of Doubling Alpha ---\n",
    "print(\"Question 2: Effect of Doubling Alpha\")\n",
    "\n",
    "def get_from_scratch_metrics(model, X_test_scaled, y_test_original, model_name):\n",
    "    # Predict on scaled test data\n",
    "    y_pred_log = model.predict(X_test_scaled)\n",
    "    # Reverse the log transform\n",
    "    y_pred_orig = np.expm1(y_pred_log) \n",
    "    \n",
    "    # Use original y_test for scoring\n",
    "    r2 = my_r2_score(y_test_original, y_pred_orig)\n",
    "    rmse = my_rmse(y_test_original, y_pred_orig)\n",
    "    num_coefs = np.sum(model.weights != 0)\n",
    "    return f\"  - {model_name}:\\n    R²: {r2:.4f} | RMSE: ${rmse:,.2f} | Features Used: {num_coefs}\"\n",
    "\n",
    "print(\"\\n--- Performance on Test Set (using original SalePrice) ---\")\n",
    "# Pass the original y_test (not log-transformed) for the final report\n",
    "print(get_from_scratch_metrics(best_ridge, X_test_scaled, y_test, f\"Ridge (alpha={best_alpha_ridge})\"))\n",
    "print(get_from_scratch_metrics(doubled_ridge, X_test_scaled, y_test, f\"Ridge (alpha={best_alpha_ridge*2})\"))\n",
    "print(get_from_scratch_metrics(best_lasso, X_test_scaled, y_test, f\"Lasso (alpha={best_alpha_lasso})\"))\n",
    "print(get_from_scratch_metrics(doubled_lasso, X_test_scaled, y_test, f\"Lasso (alpha={best_alpha_lasso*2})\"))\n",
    "\n",
    "print(\"\\n  Analysis of Doubling Alpha:\")\n",
    "print(\"  - For Ridge, doubling alpha increases the penalty, shrinking all coefficients further.\")\n",
    "print(\"  - For Lasso, doubling alpha also increases the penalty, forcing more coefficients to exactly zero (see 'Features Used').\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Question 3: Significant Variables ---\n",
    "print(\"Question 3: Significant Variables (from optimal Lasso model)\")\n",
    "lasso_coefs = best_lasso.weights\n",
    "coef_series = pd.Series(lasso_coefs, index=feature_names)\n",
    "sorted_coefs = coef_series.abs().sort_values(ascending=False)\n",
    "significant_predictors = coef_series[sorted_coefs.index].loc[lambda x: x != 0]\n",
    "\n",
    "print(f\"  - Lasso (alpha={best_alpha_lasso}) selected {len(significant_predictors)} out of {len(feature_names)} features.\")\n",
    "print(\"\\n  - Top 15 Most Important Predictors:\")\n",
    "print(significant_predictors.head(15).to_string())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Question 4: How well do they describe the price? ---\n",
    "print(\"Question 4: How well do the variables describe the price?\")\n",
    "print(\"  This is answered by the R² and RMSE metrics on our test set.\")\n",
    "r2_lasso = my_r2_score(y_test, np.expm1(best_lasso.predict(X_test_scaled)))\n",
    "rmse_lasso = my_rmse(y_test, np.expm1(best_lasso.predict(X_test_scaled)))\n",
    "print(f\"  - Our best from-scratch Lasso model explains {r2_lasso:.1%} of the variance in house prices.\")\n",
    "print(f\"  - Its average prediction error (RMSE) is ${rmse_lasso:,.2f}.\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa568b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1c46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
